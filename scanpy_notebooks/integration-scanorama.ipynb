{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating spatial data with scRNA-seq using scanorama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Giovanni Palla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial shows how to work with multiple Visium datasets and perform integration of scRNA-seq dataset with **Scanpy**. It follows the [previous tutorial](basic-spatial-analysis.ipynb) on analysis and visualization of spatial transcriptomics data.\n",
    "\n",
    "We will use **Scanorama** [paper](https://www.nature.com/articles/s41587-019-0113-3) - [code](https://github.com/brianhie/scanorama) to perform integration and label transfer. It has a convenient interface with scanpy and anndata. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import anndata as an\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scanorama\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.logging.print_header()\n",
    "sc.logging.print_versions()\n",
    "subprocess.run(['rm', '-f', 'None-requirements.txt'])\n",
    "sc.set_figure_params(facecolor=\"white\", figsize=(8, 8))\n",
    "sc.settings.verbosity = 3\n",
    "\n",
    "subprocess.run(['mkdir', '-p', 'data'])\n",
    "subprocess.run(['mkdir', '-p', 'write'])\n",
    "subprocess.run(['mkdir', '-p', 'downloaded_data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data\n",
    "\n",
    "We will use two **Visium** spatial transcriptomics dataset of the mouse brain (Sagittal), which are publicly available from the [10x genomics website](https://support.10xgenomics.com/spatial-gene-expression/datasets/).\n",
    "\n",
    "The function `datasets.visium_sge()` downloads the dataset from 10x genomics and returns an AnnData object that contains counts, images and spatial coordinates. We will calculate standards QC metrics with `pp.calculate_qc_metrics` and visualize them.\n",
    "\n",
    "When using your own Visium data, use Scanpy's `read_visium()` [function](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.read_visium.html) to import it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_spatial_anterior = sc.datasets.visium_sge(\n",
    "    sample_id=\"V1_Mouse_Brain_Sagittal_Anterior\"\n",
    ")\n",
    "adata_spatial_posterior = sc.datasets.visium_sge(\n",
    "    sample_id=\"V1_Mouse_Brain_Sagittal_Posterior\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_spatial_anterior.var_names_make_unique()\n",
    "adata_spatial_posterior.var_names_make_unique()\n",
    "sc.pp.calculate_qc_metrics(adata_spatial_anterior, inplace=True)\n",
    "sc.pp.calculate_qc_metrics(adata_spatial_posterior, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, adata in [\n",
    "    (\"anterior\", adata_spatial_anterior),\n",
    "    (\"posterior\", adata_spatial_posterior),\n",
    "]:\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(12, 3))\n",
    "    fig.suptitle(f\"Covariates for filtering: {name}\")\n",
    "\n",
    "    sns.distplot(adata.obs[\"total_counts\"], kde=False, ax=axs[0])\n",
    "    sns.distplot(\n",
    "        adata.obs[\"total_counts\"][adata.obs[\"total_counts\"] < 20000],\n",
    "        kde=False,\n",
    "        bins=40,\n",
    "        ax=axs[1],\n",
    "    )\n",
    "    sns.distplot(adata.obs[\"n_genes_by_counts\"], kde=False, bins=60, ax=axs[2])\n",
    "    sns.distplot(\n",
    "        adata.obs[\"n_genes_by_counts\"][adata.obs[\"n_genes_by_counts\"] < 4000],\n",
    "        kde=False,\n",
    "        bins=60,\n",
    "        ax=axs[3],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sc.datasets.visium_sge` downloads the *filtered* visium dataset, the output of [spaceranger](https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/what-is-space-ranger) that contains only spots within the tissue slice. Indeed, looking at standard QC metrics we can observe that the samples do not contain empty spots.\n",
    "\n",
    "We proceed to normalize Visium counts data with the built-in `normalize_total` method from Scanpy, and detect highly-variable genes (for later). As discussed previously, note that there are more sensible alternatives for normalization (see discussion in [sc-tutorial paper](https://www.embopress.org/doi/full/10.15252/msb.20188746) and more recent alternatives such as [SCTransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) or [GLM-PCA](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for adata in [\n",
    "    adata_spatial_anterior,\n",
    "    adata_spatial_posterior,\n",
    "]:\n",
    "    sc.pp.normalize_total(adata, inplace=True)\n",
    "    sc.pp.log1p(adata)\n",
    "    sc.pp.highly_variable_genes(adata, flavor=\"seurat\", n_top_genes=2000, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data integration\n",
    "\n",
    "We are now ready to perform integration of the two dataset. As mentioned before, we will be using Scanorama for that. Scanorama returns two lists, one for the integrated embeddings and one for the corrected counts, for each dataset. \n",
    "We would like to note that in this context using [BBKNN](https://scanpy.readthedocs.io/en/stable/external/scanpy.external.pp.bbknn.html) or [Ingest](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.ingest.html) is also possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adatas = [adata_spatial_anterior, adata_spatial_posterior]\n",
    "adatas_cor = scanorama.correct_scanpy(adatas, return_dimred=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will concatenate the two datasets and save the integrated embeddings in `adata_spatial.obsm['scanorama_embedding']`. Furthermore we will compute UMAP to visualize the results and qualitatively assess the data integration task.\n",
    "\n",
    "Notice that we are concatenating the two dataset with `uns_merge=\"unique\"` strategy, in order to keep both images from the visium datasets in the concatenated anndata object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_spatial = sc.concat(\n",
    "    adatas_cor,\n",
    "    label=\"library_id\",\n",
    "    uns_merge=\"unique\",\n",
    "    keys=[\n",
    "        k\n",
    "        for d in [\n",
    "            adatas_cor[0].uns[\"spatial\"],\n",
    "            adatas_cor[1].uns[\"spatial\"],\n",
    "        ]\n",
    "        for k, v in d.items()\n",
    "    ],\n",
    "    index_unique=\"-\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(adata_spatial, use_rep=\"X_scanorama\")\n",
    "sc.tl.umap(adata_spatial)\n",
    "sc.tl.leiden(adata_spatial, key_added=\"clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(\n",
    "    adata_spatial, color=[\"clusters\", \"library_id\"], palette=sc.pl.palettes.default_20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the clustering result in spatial coordinates. For that, we first need to save the cluster colors in a dictionary. We can then plot the Visium tissue fo the Anterior and Posterior Sagittal view, alongside each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_colors = dict(\n",
    "    zip([str(i) for i in range(18)], adata_spatial.uns[\"clusters_colors\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "\n",
    "for i, library in enumerate(\n",
    "    [\"V1_Mouse_Brain_Sagittal_Anterior\", \"V1_Mouse_Brain_Sagittal_Posterior\"]\n",
    "):\n",
    "    ad = adata_spatial[adata_spatial.obs.library_id == library, :].copy()\n",
    "    sc.pl.spatial(\n",
    "        ad,\n",
    "        img_key=\"hires\",\n",
    "        library_id=library,\n",
    "        color=\"clusters\",\n",
    "        size=1.5,\n",
    "        palette=[\n",
    "            v\n",
    "            for k, v in clusters_colors.items()\n",
    "            if k in ad.obs.clusters.unique().tolist()\n",
    "        ],\n",
    "        legend_loc=None,\n",
    "        show=False,\n",
    "        ax=axs[i],\n",
    "    )\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the clusters, we can clearly see the stratification of the cortical layer in both of the tissues (see the [Allen brain atlas](https://mouse.brain-map.org/experiment/thumbnails/100042147?image_type=atlas) for reference). Furthermore, it seems that the dataset integration worked well, since there is a clear continuity between clusters in the two tissues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data integration and label transfer from scRNA-seq dataset\n",
    "\n",
    "e can also perform data integration between one scRNA-seq dataset and one spatial transcriptomics dataset. Such task is particularly useful because it allows us to transfer cell type labels to the Visium dataset, which were dentified from the scRNA-seq dataset. \n",
    "\n",
    "For this task, we will be using a dataset from [Tasic et al.](https://www.nature.com/articles/s41586-018-0654-5), where the mouse cortex was profiled with smart-seq technology.\n",
    "\n",
    "The dataset can be downloaded from **[GEO](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE115746)** [count](https://ftp.ncbi.nlm.nih.gov/geo/series/GSE115nnn/GSE115746/suppl/GSE115746_cells_exon_counts.csv.gz) - [metadata](https://ftp.ncbi.nlm.nih.gov/geo/series/GSE115nnn/GSE115746/suppl/GSE115746_complete_metadata_28706-cells.csv.gz).\n",
    "Conveniently, you can also download the [pre-processed dataset in h5ad format](https://hmgubox.helmholtz-muenchen.de/f/4ef254675e2a41f89835/?dl=1).\n",
    "\n",
    "Since the dataset was generated from the mouse cortex, we will subset the visium dataset in order to select only the spots part of the cortex. Note that the integration can also be performed on the whole brain slice, but it would give rise to false positive cell type assignments and and therefore it should be interpreted with more care.\n",
    "\n",
    "The integration task will be performed with Scanorama: each Visium dataset will be integrated with the smart-seq cortex dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(['cp', '-n', '../data/adata_processed_sc.h5ad', 'downloaded_data'])\n",
    "filename = \"adata_processed_sc.h5ad\"\n",
    "\n",
    "# read the file into scanpy\n",
    "adata_cortex = sc.read(f\"./downloaded_data/{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset the spatial anndata to (approximately) selects only spots belonging to the cortex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_anterior_subset = adata_spatial_anterior[\n",
    "    adata_spatial_anterior.obsm[\"spatial\"][:, 1] < 6000, :\n",
    "]\n",
    "adata_posterior_subset = adata_spatial_posterior[\n",
    "    (adata_spatial_posterior.obsm[\"spatial\"][:, 1] < 4000)\n",
    "    & (adata_spatial_posterior.obsm[\"spatial\"][:, 0] < 6000),\n",
    "    :,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run integration with Scanorama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adatas_anterior = [adata_cortex, adata_anterior_subset]\n",
    "adatas_posterior = [adata_cortex, adata_posterior_subset]\n",
    "\n",
    "# Integration.\n",
    "adatas_cor_anterior = scanorama.correct_scanpy(adatas_anterior, return_dimred=True)\n",
    "adatas_cor_posterior = scanorama.correct_scanpy(adatas_posterior, return_dimred=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate datasets and assign integrated embeddings to anndata objects.\n",
    "\n",
    "Notice that we are concatenating datasets with the `join=\"outer\"` and `uns_merge=\"first\"` strategies. This is because we want to keep the `obsm['coords']` as well as the images of the visium datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_cortex_anterior = sc.concat(\n",
    "    adatas_cor_anterior,\n",
    "    label=\"dataset\",\n",
    "    keys=[\"smart-seq\", \"visium\"],\n",
    "    join=\"outer\",\n",
    "    uns_merge=\"first\",\n",
    ")\n",
    "adata_cortex_posterior = sc.concat(\n",
    "    adatas_cor_posterior,\n",
    "    label=\"dataset\",\n",
    "    keys=[\"smart-seq\", \"visium\"],\n",
    "    join=\"outer\",\n",
    "    uns_merge=\"first\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step, we have integrated each visium dataset in a common embedding with the scRNA-seq dataset. In such embedding space, we can compute distances between samples and use such distances as *weights* to be used for for *propagating* labels from the scRNA-seq dataset to the Visium dataset. \n",
    "\n",
    "Such approach is very similar to the `TransferData` function in Seurat (see [paper](https://www.cell.com/cell/fulltext/S0092-8674(19)30559-8)). Here, we re-implement the label transfer function with a simple python function, see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frist, let's compute cosine distances between the visium dataset and the scRNA-seq dataset, in the common embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "distances_anterior = 1 - cosine_distances(\n",
    "    adata_cortex_anterior[adata_cortex_anterior.obs.dataset == \"smart-seq\"].obsm[\n",
    "        \"X_scanorama\"\n",
    "    ],\n",
    "    adata_cortex_anterior[adata_cortex_anterior.obs.dataset == \"visium\"].obsm[\n",
    "        \"X_scanorama\"\n",
    "    ],\n",
    ")\n",
    "distances_posterior = 1 - cosine_distances(\n",
    "    adata_cortex_posterior[adata_cortex_posterior.obs.dataset == \"smart-seq\"].obsm[\n",
    "        \"X_scanorama\"\n",
    "    ],\n",
    "    adata_cortex_posterior[adata_cortex_posterior.obs.dataset == \"visium\"].obsm[\n",
    "        \"X_scanorama\"\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's propagate labels from the scRNA-seq dataset to the visium dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_transfer(dist, labels):\n",
    "    lab = pd.get_dummies(labels).to_numpy().T\n",
    "    class_prob = lab @ dist\n",
    "    norm = np.linalg.norm(class_prob, 2, axis=0)\n",
    "    class_prob = class_prob / norm\n",
    "    class_prob = (class_prob.T - class_prob.min(1)) / class_prob.ptp(1)\n",
    "    return class_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_prob_anterior = label_transfer(distances_anterior, adata_cortex.obs.cell_subclass)\n",
    "class_prob_posterior = label_transfer(\n",
    "    distances_posterior, adata_cortex.obs.cell_subclass\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `class_prob_[anterior-posterior]` objects is a numpy array of shape `(cell_type, visium_spots)` that contains *assigned weights* of each spots to each cell types. This value essentially tells us how similar that spots look like, from an expression profile perspective, to all the other annotated cell types from the scRNA-seq dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the `class_prob_[anterior-posterior]` object to a dataframe and assign it to the respective anndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_anterior_df = pd.DataFrame(\n",
    "    class_prob_anterior, columns=np.sort(adata_cortex.obs.cell_subclass.unique())\n",
    ")\n",
    "cp_posterior_df = pd.DataFrame(\n",
    "    class_prob_posterior, columns=np.sort(adata_cortex.obs.cell_subclass.unique())\n",
    ")\n",
    "\n",
    "cp_anterior_df.index = adata_anterior_subset.obs.index\n",
    "cp_posterior_df.index = adata_posterior_subset.obs.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_anterior_subset_transfer = adata_anterior_subset.copy()\n",
    "adata_anterior_subset_transfer.obs = pd.concat(\n",
    "    [adata_anterior_subset.obs, cp_anterior_df], axis=1\n",
    ")\n",
    "\n",
    "adata_posterior_subset_transfer = adata_posterior_subset.copy()\n",
    "adata_posterior_subset_transfer.obs = pd.concat(\n",
    "    [adata_posterior_subset.obs, cp_posterior_df], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are then able to explore how cell types are propagated from the scRNA-seq dataset to the visium dataset. Let's first visualize the neurons cortical layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.spatial(\n",
    "    adata_anterior_subset_transfer,\n",
    "    img_key=\"hires\",\n",
    "    color=[\"L2/3 IT\", \"L4\", \"L5 PT\", \"L6 CT\"],\n",
    "    size=1.5,\n",
    ")\n",
    "sc.pl.spatial(\n",
    "    adata_posterior_subset_transfer,\n",
    "    img_key=\"hires\",\n",
    "    color=[\"L2/3 IT\", \"L4\", \"L5 PT\", \"L6 CT\"],\n",
    "    size=1.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, it seems that this approach worked, since sequential layers of cortical neurons could be correctly identified, both in the anterior and posterior sagittal slide.\n",
    "\n",
    "We can go ahead an visualize astrocytes and oligodendrocytes as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.spatial(\n",
    "    adata_anterior_subset_transfer, img_key=\"hires\", color=[\"Oligo\", \"Astro\"], size=1.5\n",
    ")\n",
    "sc.pl.spatial(\n",
    "    adata_posterior_subset_transfer, img_key=\"hires\", color=[\"Oligo\", \"Astro\"], size=1.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we showed how to work with multiple slices in Scanpy, and perform label transfers between an annotated scRNA-seq dataset and an unannotated Visium dataset. We showed that such approach, that leverages the data integration performances of Scanorama, is useful and provide a straightforward tool for exploratory analysis.\n",
    "\n",
    "However, for the label transfer task, we advise analysts to explore more principled approaches, based on cell-type deconvolution, that are likely to provide more accurate and interpretable results. See recent approaches such as:  \n",
    "\n",
    "* **Stereoscope** [paper](https://www.biorxiv.org/content/10.1101/2019.12.13.874495v1) - [code](https://github.com/almaan/stereoscope)  \n",
    "* **AutogeneS** [paper](https://www.biorxiv.org/content/10.1101/2020.02.21.940650v1) - [code](https://github.com/theislab/AutoGeneS)  \n",
    "* **MuSiC** [paper](https://www.nature.com/articles/s41467-018-08023-x) - [code](https://github.com/xuranw/MuSiC)  \n",
    "* **CIBERSORT-X** [paper](https://www.nature.com/articles/s41587-019-0114-2) - [webtool](https://cibersortx.stanford.edu/)  \n",
    "* **Deconv-seq** [code](https://github.com/rosedu1/deconvSeq)  \n",
    "* **cell2location** [paper](https://www.biorxiv.org/content/10.1101/2020.11.15.378125v1) - [code](https://github.com/BayraktarLab/cell2location)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
